{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Полная репетиция использования разгадывателя капчи. 15 классов."
      ],
      "metadata": {
        "id": "1zeR6jL1LPEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.ops.boxes import nms\n",
        "\n",
        "import pickle\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount ('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANkw_CswWYzx",
        "outputId": "27ca0f73-9969-4919-cf81-baaafe590613"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_dir_1 = '/content/gdrive/MyDrive/Weights_ML/smartsolver_weights_1_2.pth'\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=15):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(9216, 2048),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(1024, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "def predict_one_sample(model, inputs):\n",
        "    with torch.no_grad():\n",
        "        inputs = inputs\n",
        "        model.eval()\n",
        "        logit = model(inputs).cpu()\n",
        "        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n",
        "    return probs\n",
        "\n",
        "\n",
        "your_classes = ['bucket', 'clock', 'face', 'factory', 'fire', 'flag', 'hand', 'key',\n",
        "                'lock', 'monitor', 'paper', 'person', 'scissors', 't-shirt', 'wrench']\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(your_classes)\n",
        "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "# Загрузите вашу обученную модель\n",
        "alexnet = AlexNet()\n",
        "alexnet.load_state_dict(torch.load(weights_dir_1, map_location='cpu'))\n",
        "alexnet.eval()\n",
        "\n",
        "# Загрузите label_encoder, если используете его\n",
        "label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n",
        "\n",
        "def classify_image_path(image_path, model, label_encoder=None):\n",
        "    image = Image.open(image_path)\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((227, 227)),  # Измените на ожидаемый размер\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    input_tensor = preprocess(image).unsqueeze(0)\n",
        "    probs = predict_one_sample(model, input_tensor)\n",
        "    predicted_class_idx = np.argmax(probs, axis=1)[0]\n",
        "\n",
        "    if label_encoder:\n",
        "        predicted_class = label_encoder.classes_[predicted_class_idx]\n",
        "    else:\n",
        "        predicted_class = str(predicted_class_idx)\n",
        "\n",
        "    return predicted_class"
      ],
      "metadata": {
        "id": "1lZLbnm9QAsg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_dir_2 = '/content/gdrive/MyDrive/Weights_ML/smartsolver_weights_2_4.pth'\n",
        "\n",
        "def create_model(num_classes, pretrained=False):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "device = torch.device('cpu')\n",
        "model = create_model(15 + 1)\n",
        "model.load_state_dict(torch.load(weights_dir_2, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "def detect(class_name, path,iou_threshold=0.1, threshold=0.8):\n",
        "    model.eval()\n",
        "    img = cv2.imread(path)\n",
        "    img_ = img / 255.\n",
        "    img_ = torch.from_numpy(img_).permute(2, 0, 1).unsqueeze(0).to(torch.float)\n",
        "    predict = model(img_)\n",
        "    ind = nms(predict[0]['boxes'], predict[0]['scores'], iou_threshold).detach().cpu().numpy()\n",
        "\n",
        "    class_names = [\"\", \"bucket\", \"clock\", \"face\", \"factory\", \"fire\", \"flag\", \"hand\", \"key\", \"lock\",\n",
        "                   \"monitor\", \"paper\", \"person\", \"scissors\", \"t-shirt\", \"wrench\"]\n",
        "\n",
        "    class_idx = class_names.index(class_name)\n",
        "\n",
        "    for i, box in enumerate(predict[0]['boxes'][ind]):\n",
        "        if predict[0]['scores'][i] > threshold and int(predict[0]['labels'][i]) == class_idx:\n",
        "            center_x = int((box[0] + box[2]) / 2)\n",
        "            center_y = int((box[1] + box[3]) / 2)\n",
        "            return center_x, center_y\n",
        "\n",
        "    return None, None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyuolQJDHnlm",
        "outputId": "0917d136-2a0e-407b-91fb-508cfa80b873"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pics_path = ['/content/img68.jpg', '/content/img14.jpg', '/content/img6.jpg']\n",
        "\n",
        "start = time.time()\n",
        "# Пример использования 42 flag-68 monitor-14 factory-6\n",
        "for i in range(len(pics_path)):\n",
        "    classify = classify_image_path(pics_path[i], alexnet, label_encoder)\n",
        "    coords = detect(classify,'/content/0042.jpg')\n",
        "    print(coords)\n",
        "end = time.time()\n",
        "print(f\"Потрачено {round((end - start) , 1)} секунд\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_RCQw60TwN7",
        "outputId": "a8393c57-dcc3-4691-a97f-da403df50a1b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(313, 329)\n",
            "(221, 700)\n",
            "(1261, 350)\n",
            "Потрачено 18.3 секунд\n"
          ]
        }
      ]
    }
  ]
}